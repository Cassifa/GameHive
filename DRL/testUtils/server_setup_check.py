#!/usr/bin/env python3
"""
æœåŠ¡å™¨ç¯å¢ƒæ£€æŸ¥è„šæœ¬
éªŒè¯PyTorch 2.5.1 + CUDA 12.4 + Python 3.12ç¯å¢ƒ
"""

import sys
import platform
import subprocess

def check_system_info():
    """æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯"""
    print("=" * 60)
    print("ğŸ–¥ï¸ ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥")
    print("=" * 60)
    
    print(f"æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
    print(f"Pythonç‰ˆæœ¬: {platform.python_version()}")
    print(f"æ¶æ„: {platform.machine()}")
    
    # æ£€æŸ¥CUDA
    try:
        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            cuda_version = result.stdout.split('release ')[1].split(',')[0]
            print(f"CUDAç‰ˆæœ¬: {cuda_version}")
        else:
            print("âŒ CUDAæœªå®‰è£…æˆ–ä¸å¯ç”¨")
    except FileNotFoundError:
        print("âŒ nvccå‘½ä»¤æœªæ‰¾åˆ°ï¼ŒCUDAå¯èƒ½æœªå®‰è£…")
    
    # æ£€æŸ¥GPU
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            lines = result.stdout.split('\n')
            for line in lines:
                if 'Driver Version:' in line:
                    driver_version = line.split('Driver Version: ')[1].split()[0]
                    print(f"NVIDIAé©±åŠ¨: {driver_version}")
                    break
        else:
            print("âŒ nvidia-smiå‘½ä»¤å¤±è´¥")
    except FileNotFoundError:
        print("âŒ nvidia-smiå‘½ä»¤æœªæ‰¾åˆ°")

def check_pytorch():
    """æ£€æŸ¥PyTorchç¯å¢ƒ"""
    print("\n" + "=" * 60)
    print("ğŸ”¥ PyTorchç¯å¢ƒæ£€æŸ¥")
    print("=" * 60)
    
    try:
        import torch
        print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        # æ£€æŸ¥CUDAæ”¯æŒ
        cuda_available = torch.cuda.is_available()
        print(f"âœ… CUDAå¯ç”¨: {cuda_available}")
        
        if cuda_available:
            print(f"âœ… CUDAç‰ˆæœ¬: {torch.version.cuda}")
            print(f"âœ… GPUæ•°é‡: {torch.cuda.device_count()}")
            
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"âœ… GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
        else:
            print("âŒ CUDAä¸å¯ç”¨")
            return False
            
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")
        return False
    
    return True

def check_dependencies():
    """æ£€æŸ¥å…¶ä»–ä¾èµ–"""
    print("\n" + "=" * 60)
    print("ğŸ“¦ ä¾èµ–åŒ…æ£€æŸ¥")
    print("=" * 60)
    
    dependencies = ['numpy', 'matplotlib', 'onnx', 'onnxruntime']
    
    for dep in dependencies:
        try:
            __import__(dep)
            print(f"âœ… {dep}: å·²å®‰è£…")
        except ImportError:
            print(f"âŒ {dep}: æœªå®‰è£…")

def test_gpu_performance():
    """æµ‹è¯•GPUæ€§èƒ½"""
    print("\n" + "=" * 60)
    print("âš¡ GPUæ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    
    try:
        import torch
        import time
        
        if not torch.cuda.is_available():
            print("âŒ è·³è¿‡GPUæµ‹è¯•ï¼ˆCUDAä¸å¯ç”¨ï¼‰")
            return
        
        device = torch.device('cuda')
        
        # æµ‹è¯•çŸ©é˜µä¹˜æ³•æ€§èƒ½
        print("ğŸ”„ æµ‹è¯•GPUè®¡ç®—æ€§èƒ½...")
        size = 4096
        a = torch.randn(size, size, device=device)
        b = torch.randn(size, size, device=device)
        
        # é¢„çƒ­
        for _ in range(3):
            _ = torch.mm(a, b)
        torch.cuda.synchronize()
        
        # æ€§èƒ½æµ‹è¯•
        start_time = time.time()
        for _ in range(10):
            c = torch.mm(a, b)
        torch.cuda.synchronize()
        end_time = time.time()
        
        avg_time = (end_time - start_time) / 10
        gflops = (2 * size**3) / (avg_time * 1e9)
        
        print(f"âœ… çŸ©é˜µä¹˜æ³•æ€§èƒ½: {avg_time:.3f}ç§’")
        print(f"âœ… è®¡ç®—æ€§èƒ½: {gflops:.1f} GFLOPS")
        
        # å†…å­˜æµ‹è¯•
        memory_allocated = torch.cuda.memory_allocated() / 1024**3
        memory_reserved = torch.cuda.memory_reserved() / 1024**3
        print(f"âœ… GPUå†…å­˜ä½¿ç”¨: {memory_allocated:.2f}GB / {memory_reserved:.2f}GB")
        
    except Exception as e:
        print(f"âŒ GPUæµ‹è¯•å¤±è´¥: {e}")

def check_training_readiness():
    """æ£€æŸ¥è®­ç»ƒå°±ç»ªçŠ¶æ€"""
    print("\n" + "=" * 60)
    print("ğŸ¯ è®­ç»ƒå°±ç»ªæ£€æŸ¥")
    print("=" * 60)
    
    # æ£€æŸ¥é¡¹ç›®æ–‡ä»¶
    import os
    required_files = [
        'config.py', 'board.py', 'policy_network.py', 
        'tree_node.py', 'mcts.py', 'game.py', 
        'trainer.py', 'train.py'
    ]
    
    missing_files = []
    for file in required_files:
        if os.path.exists(f'../{file}'):
            print(f"âœ… {file}: å­˜åœ¨")
        else:
            print(f"âŒ {file}: ç¼ºå¤±")
            missing_files.append(file)
    
    if missing_files:
        print(f"\nâŒ ç¼ºå¤±å…³é”®æ–‡ä»¶: {missing_files}")
        return False
    
    print("\nâœ… æ‰€æœ‰è®­ç»ƒæ–‡ä»¶å°±ç»ªï¼")
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æœåŠ¡å™¨ç¯å¢ƒæ£€æŸ¥ - PyTorch 2.5.1 + CUDA 12.4")
    
    # ç³»ç»Ÿæ£€æŸ¥
    check_system_info()
    
    # PyTorchæ£€æŸ¥
    pytorch_ok = check_pytorch()
    
    # ä¾èµ–æ£€æŸ¥
    check_dependencies()
    
    # GPUæ€§èƒ½æµ‹è¯•
    if pytorch_ok:
        test_gpu_performance()
    
    # è®­ç»ƒå°±ç»ªæ£€æŸ¥
    training_ready = check_training_readiness()
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“‹ æ£€æŸ¥æ€»ç»“")
    print("=" * 60)
    
    if pytorch_ok and training_ready:
        print("ğŸ‰ ç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼å¯ä»¥å¼€å§‹10ä¸‡æ¬¡è®­ç»ƒ")
        print("\næ¨èè®­ç»ƒå‘½ä»¤:")
        print("python train.py --max_games 100000 --batch_size 4096 --mcts_simulations 200 --use_gpu")
    else:
        print("âš ï¸ ç¯å¢ƒå­˜åœ¨é—®é¢˜ï¼Œè¯·å…ˆè§£å†³ä¸Šè¿°é—®é¢˜")

if __name__ == "__main__":
    main() 