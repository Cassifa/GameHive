#!/usr/bin/env python3
"""
æµ‹è¯•è„šæœ¬ï¼šéªŒè¯PyTorch AlphaGo Zeroå®ç°æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import torch
import numpy as np
import sys
import os

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ä¸»é¡¹ç›®æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import Config
from board import Board
from policy_network import PolicyValueNet
from mcts import MCTSPlayer
from game import Game

def test_board():
    """æµ‹è¯•æ£‹ç›˜åŠŸèƒ½"""
    print("æµ‹è¯•æ£‹ç›˜åŠŸèƒ½...")
    board = Board()
    board.init_board()
    
    # æµ‹è¯•åŸºæœ¬æ“ä½œ
    assert len(board.availables) == 64
    assert board.current_player == 1
    
    # æµ‹è¯•è½å­
    board.do_move(28)  # ä¸­å¿ƒä½ç½®
    assert 28 not in board.availables
    assert board.current_player == 2
    
    # æµ‹è¯•çŠ¶æ€è¡¨ç¤º
    state = board.current_state()
    assert state.shape == (4, 8, 8)
    
    print("âœ“ æ£‹ç›˜åŠŸèƒ½æµ‹è¯•é€šè¿‡")

def test_network():
    """æµ‹è¯•ç¥ç»ç½‘ç»œ"""
    print("æµ‹è¯•ç¥ç»ç½‘ç»œ...")
    
    # åˆ›å»ºç½‘ç»œ
    net = PolicyValueNet()
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    dummy_input = torch.randn(1, 4, 8, 8)
    policy, value = net(dummy_input)
    
    assert policy.shape == (1, 64)
    assert value.shape == (1, 1)
    assert torch.allclose(policy.sum(dim=1), torch.ones(1), atol=1e-6)  # æ¦‚ç‡å’Œä¸º1
    assert -1 <= value.item() <= 1  # ä»·å€¼åœ¨[-1,1]èŒƒå›´å†…
    
    print("âœ“ ç¥ç»ç½‘ç»œæµ‹è¯•é€šè¿‡")

def test_mcts():
    """æµ‹è¯•MCTS"""
    print("æµ‹è¯•MCTS...")
    
    board = Board()
    board.init_board()
    
    # åˆ›å»ºç®€å•çš„ç­–ç•¥å‡½æ•°
    def simple_policy(board):
        n = len(board.availables)
        probs = np.ones(n) / n
        return zip(board.availables, probs), 0.0
    
    # åˆ›å»ºMCTSç©å®¶
    player = MCTSPlayer(simple_policy, n_playout=10, is_selfplay=1)
    
    # æµ‹è¯•è·å–åŠ¨ä½œ
    move, move_probs = player.get_action(board, return_prob=1)
    
    assert move in board.availables
    assert len(move_probs) == 64
    assert abs(move_probs.sum() - 1.0) < 1e-6
    
    print("âœ“ MCTSæµ‹è¯•é€šè¿‡")

def test_game():
    """æµ‹è¯•æ¸¸æˆé€»è¾‘"""
    print("æµ‹è¯•æ¸¸æˆé€»è¾‘...")
    
    game = Game()
    
    # åˆ›å»ºç®€å•ç­–ç•¥
    def simple_policy(board):
        n = len(board.availables)
        probs = np.ones(n) / n
        return zip(board.availables, probs), 0.0
    
    player = MCTSPlayer(simple_policy, n_playout=5, is_selfplay=1)
    
    # æµ‹è¯•è‡ªæˆ‘å¯¹å¼ˆï¼ˆé™åˆ¶æ­¥æ•°é¿å…æ— é™å¾ªç¯ï¼‰
    game.board.init_board()
    states, mcts_probs, current_players = [], [], []
    
    for _ in range(5):  # åªæµ‹è¯•5æ­¥
        if game.board.game_end()[0]:
            break
            
        move, move_probs = player.get_action(game.board, temp=1.0, return_prob=1)
        states.append(game.board.current_state())
        mcts_probs.append(move_probs)
        current_players.append(game.board.current_player)
        game.board.do_move(move)
    
    assert len(states) > 0
    assert len(mcts_probs) > 0
    assert len(current_players) > 0
    
    print("âœ“ æ¸¸æˆé€»è¾‘æµ‹è¯•é€šè¿‡")

def test_integration():
    """é›†æˆæµ‹è¯•"""
    print("æµ‹è¯•å®Œæ•´é›†æˆ...")
    
    # æµ‹è¯•ç½‘ç»œä¸MCTSçš„é›†æˆ
    net = PolicyValueNet()
    board = Board()
    board.init_board()
    
    # æµ‹è¯•ç­–ç•¥ä»·å€¼å‡½æ•°
    act_probs, value = net.policy_value_fn(board)
    act_probs = list(act_probs)
    
    assert len(act_probs) == len(board.availables)
    assert -1 <= value <= 1
    
    # æµ‹è¯•MCTSä¸ç½‘ç»œçš„é›†æˆ
    player = MCTSPlayer(net.policy_value_fn, n_playout=5, is_selfplay=1)
    move, move_probs = player.get_action(board, return_prob=1)
    
    assert move in board.availables
    assert len(move_probs) == 64
    
    print("âœ“ é›†æˆæµ‹è¯•é€šè¿‡")

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("=" * 50)
    print("PyTorch AlphaGo Zero å®ç°æµ‹è¯•")
    print("=" * 50)
    
    try:
        test_board()
        test_network()
        test_mcts()
        test_game()
        test_integration()
        
        print("\n" + "=" * 50)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ä»£ç å¯ä»¥æ­£å¸¸è¿è¡Œã€‚")
        print("=" * 50)
        
        # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
        print(f"\nè®¾å¤‡ä¿¡æ¯:")
        print(f"  PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"  CUDAå¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"  GPUè®¾å¤‡: {torch.cuda.get_device_name()}")
        print(f"  å½“å‰è®¾å¤‡: {Config.DEVICE}")
        
        print(f"\nç½‘ç»œå‚æ•°:")
        net = PolicyValueNet()
        total_params = sum(p.numel() for p in net.parameters())
        trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)
        print(f"  æ€»å‚æ•°æ•°: {total_params:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == "__main__":
    main() 