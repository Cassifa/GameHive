#!/usr/bin/env python3
"""
GPUè®­ç»ƒè¦æ±‚æ£€æµ‹è„šæœ¬
æ£€æŸ¥ç³»ç»Ÿæ˜¯å¦æ»¡è¶³GPUè®­ç»ƒçš„æ¡ä»¶
"""

import sys
import os
sys.path.append('..')

import torch
import platform
from config import Config

def check_gpu_requirements():
    """æ£€æŸ¥GPUè®­ç»ƒè¦æ±‚"""
    print("=" * 70)
    print("ğŸ® GPUè®­ç»ƒè¦æ±‚æ£€æµ‹")
    print("=" * 70)
    
    # 1. ç³»ç»Ÿä¿¡æ¯
    print("\nğŸ“‹ ç³»ç»Ÿä¿¡æ¯:")
    print(f"  æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
    print(f"  Pythonç‰ˆæœ¬: {platform.python_version()}")
    print(f"  PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    # 2. CUDAæ£€æµ‹
    print("\nğŸ” CUDAæ£€æµ‹:")
    cuda_available = torch.cuda.is_available()
    print(f"  CUDAå¯ç”¨: {cuda_available}")
    
    if cuda_available:
        print(f"  CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"  cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        gpu_count = torch.cuda.device_count()
        print(f"  GPUæ•°é‡: {gpu_count}")
        
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
    else:
        print("  âŒ æœªæ£€æµ‹åˆ°CUDAæ”¯æŒ")
    
    # 3. GPUè®­ç»ƒæ¡ä»¶æ£€æŸ¥
    print("\nâœ… GPUè®­ç»ƒæ¡ä»¶æ£€æŸ¥:")
    
    conditions = []
    
    # æ¡ä»¶1: CUDAå¯ç”¨
    if cuda_available:
        conditions.append("âœ… CUDAæ”¯æŒ: å·²å®‰è£…")
    else:
        conditions.append("âŒ CUDAæ”¯æŒ: æœªå®‰è£…")
    
    # æ¡ä»¶2: GPUå†…å­˜
    if cuda_available and gpu_count > 0:
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        if gpu_memory >= 4:
            conditions.append(f"âœ… GPUå†…å­˜: {gpu_memory:.1f}GB (æ¨è4GB+)")
        else:
            conditions.append(f"âš ï¸ GPUå†…å­˜: {gpu_memory:.1f}GB (å»ºè®®4GB+)")
    else:
        conditions.append("âŒ GPUå†…å­˜: æ— GPUè®¾å¤‡")
    
    # æ¡ä»¶3: PyTorch CUDAç‰ˆæœ¬
    if hasattr(torch.version, 'cuda') and torch.version.cuda:
        conditions.append(f"âœ… PyTorch CUDAç‰ˆæœ¬: {torch.version.cuda}")
    else:
        conditions.append("âŒ PyTorch CUDAç‰ˆæœ¬: CPUç‰ˆæœ¬")
    
    for condition in conditions:
        print(f"  {condition}")
    
    # 4. è®­ç»ƒå»ºè®®
    print("\nğŸ’¡ è®­ç»ƒå»ºè®®:")
    
    if cuda_available:
        print("  ğŸ‰ æ‚¨çš„ç³»ç»Ÿæ”¯æŒGPUè®­ç»ƒ!")
        print("  ğŸ“ ä½¿ç”¨GPUè®­ç»ƒçš„å‘½ä»¤:")
        print("     python train.py --use_gpu")
        print("     python train.py  # é»˜è®¤ä¼šè‡ªåŠ¨ä½¿ç”¨GPU")
        
        if gpu_count > 0:
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            if gpu_memory >= 8:
                print("  ğŸš€ æ¨èé…ç½® (é«˜æ€§èƒ½):")
                print("     --batch_size 4096")
                print("     --mcts_simulations 200")
            elif gpu_memory >= 4:
                print("  âš–ï¸ æ¨èé…ç½® (å¹³è¡¡):")
                print("     --batch_size 2048")
                print("     --mcts_simulations 100")
            else:
                print("  ğŸ’¾ æ¨èé…ç½® (èŠ‚çœå†…å­˜):")
                print("     --batch_size 1024")
                print("     --mcts_simulations 50")
    else:
        print("  âš ï¸ æ‚¨çš„ç³»ç»Ÿä¸æ”¯æŒGPUè®­ç»ƒ")
        print("  ğŸ“ ä½¿ç”¨CPUè®­ç»ƒçš„å‘½ä»¤:")
        print("     python train.py --use_cpu")
        print("  ğŸ’¾ CPUè®­ç»ƒæ¨èé…ç½®:")
        print("     --batch_size 512")
        print("     --mcts_simulations 50")
        print("     --max_games 500  # å‡å°‘è®­ç»ƒé‡")
    
    return cuda_available

def explain_gpu_requirements():
    """è¯¦ç»†è§£é‡ŠGPUè®­ç»ƒè¦æ±‚"""
    print("\n" + "=" * 70)
    print("ğŸ“š GPUè®­ç»ƒè¦æ±‚è¯¦è§£")
    print("=" * 70)
    
    print("\nğŸ”§ ç¡¬ä»¶è¦æ±‚:")
    print("  1. NVIDIA GPU (æ”¯æŒCUDA)")
    print("     - æ¨è: GTX 1060 6GB æˆ–æ›´å¥½")
    print("     - æœ€ä½: GTX 1050 Ti 4GB")
    print("     - ä¸æ”¯æŒ: AMD GPU, Intelé›†æ˜¾")
    
    print("\n  2. GPUå†…å­˜è¦æ±‚:")
    print("     - 4GB: åŸºç¡€è®­ç»ƒ (batch_size=1024)")
    print("     - 6GB: æ ‡å‡†è®­ç»ƒ (batch_size=2048)")
    print("     - 8GB+: é«˜æ€§èƒ½è®­ç»ƒ (batch_size=4096)")
    
    print("\nğŸ’¿ è½¯ä»¶è¦æ±‚:")
    print("  1. NVIDIAé©±åŠ¨ç¨‹åº")
    print("     - ç‰ˆæœ¬: 450.80.02+ (Linux) æˆ– 452.39+ (Windows)")
    print("     - æ£€æŸ¥å‘½ä»¤: nvidia-smi")
    
    print("\n  2. CUDA Toolkit")
    print("     - ç‰ˆæœ¬: 11.8 æˆ– 12.1")
    print("     - ä¸‹è½½: https://developer.nvidia.com/cuda-downloads")
    
    print("\n  3. PyTorch CUDAç‰ˆæœ¬")
    print("     - å½“å‰ç‰ˆæœ¬: CPUç‰ˆæœ¬ (ä¸æ”¯æŒGPU)")
    print("     - éœ€è¦å®‰è£…: GPUç‰ˆæœ¬")
    print("     - å®‰è£…å‘½ä»¤:")
    print("       pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
    
    print("\nğŸš€ æ€§èƒ½å¯¹æ¯”:")
    print("  è®­ç»ƒé€Ÿåº¦ (ç›¸å¯¹CPU):")
    print("     - GTX 1060 6GB: 5-8å€")
    print("     - RTX 3060 12GB: 10-15å€")
    print("     - RTX 4080 16GB: 20-30å€")
    
    print("\nâš ï¸ å¸¸è§é—®é¢˜:")
    print("  1. 'CUDA out of memory'")
    print("     è§£å†³: å‡å° --batch_size")
    
    print("\n  2. 'CUDA not available'")
    print("     è§£å†³: é‡æ–°å®‰è£…PyTorch GPUç‰ˆæœ¬")
    
    print("\n  3. è®­ç»ƒå¾ˆæ…¢")
    print("     æ£€æŸ¥: æ˜¯å¦çœŸçš„åœ¨ä½¿ç”¨GPU")

def test_gpu_training():
    """æµ‹è¯•GPUè®­ç»ƒåŠŸèƒ½"""
    print("\n" + "=" * 70)
    print("ğŸ§ª GPUè®­ç»ƒåŠŸèƒ½æµ‹è¯•")
    print("=" * 70)
    
    if not torch.cuda.is_available():
        print("  âŒ è·³è¿‡GPUæµ‹è¯• (CUDAä¸å¯ç”¨)")
        return False
    
    try:
        print("  ğŸ”„ åˆ›å»ºæµ‹è¯•å¼ é‡...")
        device = torch.device('cuda')
        x = torch.randn(100, 100).to(device)
        y = torch.randn(100, 100).to(device)
        
        print("  ğŸ”„ æ‰§è¡ŒGPUè®¡ç®—...")
        z = torch.mm(x, y)
        
        print("  âœ… GPUè®¡ç®—æˆåŠŸ!")
        print(f"  ğŸ“Š ç»“æœå½¢çŠ¶: {z.shape}")
        print(f"  ğŸ’¾ GPUå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**2:.1f}MB")
        
        # æ¸…ç†GPUå†…å­˜
        del x, y, z
        torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"  âŒ GPUæµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥GPUè¦æ±‚
    gpu_available = check_gpu_requirements()
    
    # è¯¦ç»†è§£é‡Šè¦æ±‚
    explain_gpu_requirements()
    
    # æµ‹è¯•GPUåŠŸèƒ½
    if gpu_available:
        test_gpu_training()
    
    print("\n" + "=" * 70)
    print("ğŸ“‹ æ€»ç»“")
    print("=" * 70)
    
    if gpu_available:
        print("  ğŸ‰ æ­å–œ! æ‚¨çš„ç³»ç»Ÿæ”¯æŒGPUè®­ç»ƒ")
        print("  ğŸš€ å»ºè®®ä½¿ç”¨: python train.py --use_gpu")
    else:
        print("  âš ï¸ æ‚¨çš„ç³»ç»Ÿç›®å‰ä¸æ”¯æŒGPUè®­ç»ƒ")
        print("  ğŸ’» å»ºè®®ä½¿ç”¨: python train.py --use_cpu")
        print("  ğŸ“ å¦‚éœ€GPUè®­ç»ƒï¼Œè¯·å®‰è£…CUDAå’ŒPyTorch GPUç‰ˆæœ¬")

if __name__ == "__main__":
    main() 